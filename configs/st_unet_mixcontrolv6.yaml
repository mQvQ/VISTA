model:
  base_learning_rate: 2.5e-05
  target: ldm.models.diffusion.st_ldm.STLatentDiffusion
  params:
    gen_joint: False
    load_only_unet: false
    # ckpt_path: /data1/tma/ST-Diffusion/st-logs/01-19T05-40_single_st_unet_imgcond_hest_benchmark_visium_v5_gae_emb/st-single-unet/01-19T05-40_single_st_unet_imgcond_hest_benchmark_visium_v5_gae_emb/checkpoints/epoch=54-step=99999.ckpt
    linear_start: 0.0015
    linear_end: 0.0195
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: image
    cond_stage_key: conds
    st_size:
      - 1
      - 256
    channels: 1
    cond_stage_trainable: true
    conditioning_key: crossattn
    monitor: val/loss
    use_ema: true
    scheduler_config:
      target: ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps:
        - 1000
        cycle_lengths:
        - 10000000000000
        f_start:
        - 1.0e-06
        f_max:
        - 1.0
        f_min:
        - 1.0
    unet_config:
      target: ldm.modules.diffusionmodules.st_unet_v2.STUNetModel
      params:
        st_size: # mlp
        - 1
        - 512
        input_st_num: 256
        in_channels: 1
        model_channels: 192
        st_out_channels: 1
        attention_resolutions:
        - 8
        - 4
        - 2
        num_res_blocks: 1
        channel_mult:
        - 1
        - 2
        - 3
        - 5
        num_heads: 2
        use_spatial_transformer: true
        transformer_depth: 1
        context_dim: 768
        use_scale_shift_norm: true
        use_checkpoint: true
    first_stage_config:
      target: ldm.models.autoencoder.VQModelInterface
      params:
        # ckpt_path: models/first_stage_models/vq-f4/model.ckpt
        embed_dim: 3
        n_embed: 8192
        ddconfig:
          double_z: false
          z_channels: 3
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    cond_stage_config:
      target: ldm.modules.encoders.modules.MultiControlEncoderv6
      params:
        image_cond_config:
          version: conch_ViT-B-16
          checkpoint_path: /home/tma/VISTA/CONCH/checkpoints/conch/pytorch_model.bin
        celltype_cond_config:
          num_celltypes:  81 # 80 + 1
          prototype_dim: 768
        num_classes: 6
data:
  target: st_main.DataModuleFromConfig
  params:
    batch_size: 32
    num_workers: 8
    wrap: false
    train:
      target: ldm.data.joint_diff.st_emb_label_cellanno_aug_dataset.STEmbDataset
      params:
        config:
          root: /disk2/tma/VISTA/hest_benchmark_visium_v5
          use_celltype: True
          split: train
          emb_path: multi_organ_gae_train_set_log1p_d256_embs_3+1.pkl
          prompts_file: None
          st_size: 256
          crop_size: 256
          p_uncond: 0.1
    validation:
      target: ldm.data.joint_diff.st_emb_label_cellanno_aug_dataset.STEmbDataset
      params:
        config:
          root: /disk2/tma/VISTA/hest_benchmark_visium_v5
          use_celltype: True
          split: test
          emb_path: multi_organ_gae_test_set_log1p_d256_embs_3+1.pkl
          st_size: 256
          crop_size: 256
          p_uncond: 0.1

lightning:
  callbacks:
    model_checkpoint:
      target: pytorch_lightning.callbacks.ModelCheckpoint
      params:
        save_weights_only: true
  trainer:
    benchmark: true
    accumulate_grad_batches: 1
    max_steps: 100000
    fast_dev_run: False
    limit_val_batches: 0.2
    accelerator: ddp
    check_val_every_n_epoch: 10
    log_every_n_steps: 100

